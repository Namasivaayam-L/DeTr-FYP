{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import argparse\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","import time\n","import os\n","from detr_tf.data.coco import load_coco_dataset\n","from detr_tf.networks.detr import get_detr_model\n","from detr_tf.optimizers import setup_optimizers\n","from detr_tf.optimizers import gather_gradient, aggregate_grad_and_apply\n","from detr_tf.logger.training_logging import train_log, valid_log\n","from detr_tf.loss.loss import get_losses\n","from detr_tf.inference import numpy_bbox_to_image\n","from detr_tf.training_config import TrainingConfig, training_config_parser\n","from detr_tf import training\n","import time\n","import tensorflow as tf\n","from random import shuffle\n","import pandas as pd\n","import numpy as np\n","import imageio.v2 as imageio\n","import os\n","from detr_tf.data import processing\n","from detr_tf.data.transformation import detr_transform\n","from detr_tf import bbox\n","from detr_tf.networks.resnet_backbone import ResNet50Backbone\n","from detr_tf.networks.custom_layers import Linear, FixedEmbedding\n","from detr_tf.networks.position_embeddings import PositionEmbeddingSine\n","from detr_tf.networks.transformer import Transformer\n","from detr_tf.networks.transformer import TransformerEncoder\n","from detr_tf.networks.transformer import TransformerDecoder\n","from detr_tf.networks.transformer import EncoderLayer\n","from detr_tf.networks.transformer import DecoderLayer\n","from detr_tf.networks.transformer import MultiHeadAttention\n","from tensorflow.python.ops.resource_variable_ops import VariableSpec"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["custom_objects = {\n","'ResNet50Backbone':ResNet50Backbone,\n","'PositionEmbeddingSine':PositionEmbeddingSine,\n","'Transformer':Transformer,\n","'TransformerEncoder':TransformerEncoder,\n","'TransformerDecoder':TransformerDecoder,\n","'EncoderLayer':EncoderLayer,\n","'DecoderLayer':DecoderLayer,\n","'MultiHeadAttention':MultiHeadAttention,\n","'Linear':Linear,\n","'FixedEmbedding':FixedEmbedding,\n","'VariableSpec':VariableSpec\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_data_from_index(index, class_names, filenames, anns, config, augmentation, img_dir):\n","    # Open the image\n","    image = imageio.imread(os.path.join(config.data_dir, img_dir, filenames[index]))\n","    # Select all the annotatiom (bbox and class) on this image\n","    image_anns = anns[anns[\"filename\"] == filenames[index]]    \n","    \n","    # Convert all string class to number (the target class)\n","    t_class = image_anns[\"class\"].map(lambda x: class_names.index(x)).to_numpy()\n","    # Select the width&height of each image (should be the same since all the ann belongs to the same image)\n","    width = image_anns[\"width\"].to_numpy()\n","    height = image_anns[\"height\"].to_numpy()\n","    # Select the xmin, ymin, xmax and ymax of each bbox, Then, normalized the bbox to be between and 0 and 1\n","    # Finally, convert the bbox from xmin,ymin,xmax,ymax to x_center,y_center,width,height\n","    bbox_list = image_anns[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].to_numpy()\n","    bbox_list = bbox_list / [width[0], height[0], width[0], height[0]] \n","    t_bbox = bbox.xy_min_xy_max_to_xcycwh(bbox_list)\n","    \n","    # Transform and augment image with bbox and class if needed\n","    image, t_bbox, t_class = detr_transform(image, t_bbox, t_class, config, augmentation=augmentation)\n","\n","    # Normalized image\n","    image = processing.normalized_images(image, config)\n","\n","    return image.astype(np.float32), t_bbox.astype(np.float32), np.expand_dims(t_class, axis=-1).astype(np.int64)\n","\n","def load_tfcsv_dataset(config, batch_size, augmentation=False, exclude=[], ann_dir=None, ann_file=None, img_dir=None):\n","    \"\"\" Load the hardhat dataset\n","    \"\"\"\n","    ann_dir = config.ann_dir if ann_dir is None else ann_dir\n","    ann_file = config.ann_file if ann_file is None else ann_file\n","    img_dir = config.img_dir if img_dir is None else img_dir\n","    anns = pd.read_csv(os.path.join(config.data_dir, img_dir, ann_file)).head(4000)\n","    for name  in exclude:\n","        anns = anns[anns[\"class\"] != name]\n","\n","    unique_class = anns[\"class\"].unique()\n","    unique_class.sort()\n","    \n","\n","    # Set the background class to 0\n","    config.background_class = 0\n","    class_names = [\"background\"] + unique_class.tolist()\n","\n","\n","    filenames = anns[\"filename\"].unique().tolist()\n","    indexes = list(range(0, len(filenames)))\n","    shuffle(indexes)\n","\n","    dataset = tf.data.Dataset.from_tensor_slices(indexes)\n","    dataset = dataset.map(lambda idx: processing.numpy_fc(\n","        idx, load_data_from_index, \n","        class_names=class_names, filenames=filenames, anns=anns, config=config, augmentation=augmentation, img_dir=img_dir)\n","    ,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    \n","\n","    # Filter labels to be sure to keep only sample with at least one bbox\n","    dataset = dataset.filter(lambda imgs, tbbox, tclass: tf.shape(tbbox)[0] > 0)\n","    # Pad bbox and labels\n","    dataset = dataset.map(processing.pad_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    # Batch images\n","    dataset = dataset.batch(batch_size, drop_remainder=True)\n","    \n","    return dataset, class_names\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_model(config):\n","    \"\"\" Build the model with the pretrained weights. In this example\n","    we do not add new layers since the pretrained model is already trained on coco.\n","    See examples/finetuning_voc.py to add new layers.\n","    \"\"\"\n","    # Load detr model without weight. \n","    # Use the tensorflow backbone with the imagenet weights\n","    # detr = get_detr_model(config, include_top=True, nb_class=10, weights=None, tf_backbone=True, custom_objects=custom_objects)\n","    detr = get_detr_model(config, include_top=False, nb_class=10, weights='model/detr-weights/detr3.ckpt', custom_objects=custom_objects)\n","    detr.summary()\n","    return detr\n","\n","def run_finetuning(config):\n","    # Load the model with the new layers to finetune\n","    detr = build_model(config)\n","    # Load the training and validation dataset\n","    train_dt, coco_class_names = load_tfcsv_dataset(config, config.batch_size, augmentation=True, img_dir='test', ann_file='test.csv')\n","    valid_dt, _ = load_tfcsv_dataset(config, 1, augmentation=False, img_dir='test',ann_file='test.csv')\n","\n","    # Train the backbone and the transformers\n","    # Check the training_config file for the other hyperparameters\n","    config.train_backbone = True\n","    config.train_transformers = True\n","\n","    # Setup the optimziers and the trainable variables\n","    optimzers = setup_optimizers(detr, config)\n","    model_path = 'model/detr-weights/detr3.ckpt'\n","    # Run the training for 100 epochs\n","    for epoch_nb in range(1):\n","        print(\"EPOCH :\",epoch_nb)\n","        training.eval(detr, valid_dt, config, coco_class_names, evaluation_step=200)\n","        training.fit(detr, train_dt, optimzers, config, epoch_nb, coco_class_names)\n","        detr.save_weights(model_path) \n","    # tf.keras.models.save_model(detr,model_path,custom_objects)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == \"__main__\":\n","\n","    physical_devices = tf.config.list_physical_devices('GPU')\n","    if len(physical_devices) == 1:\n","        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","\n","    config = TrainingConfig()\n","    args = training_config_parser().parse_args()\n","    config.update_from_args(args)\n","\n","    # if config.log:\n","    #     wandb.init(project=\"detr-tensorflow\", reinit=True)\n","        \n","    # Run training\n","    run_finetuning(config)\n"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
